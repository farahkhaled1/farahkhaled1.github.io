{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))\n",
    "\n",
    "words = ['generous','generation','generously','generate']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))\n",
    "    print(word+' --> '+p_stemmer.stem(word))\n",
    "    print('---------------------------------------')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer , LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ls =  LancasterStemmer()\n",
    "\n",
    "words = [\"is\",\"was\",\"be\",\"been\",\"are\",\"were\"]\n",
    "\n",
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ps.stem(w)}')\n",
    "\n",
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ls.stem(w)}')\n",
    "\n",
    "words = [\"book\",\"booking\",\"booked\",\"books\",\"booker\",\"bookstore\"]\n",
    "\n",
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ps.stem(w)}')\n",
    "\n",
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ls.stem(w)}')\n",
    "\n",
    "sentence = 'had you booked the air booking yet ? if not try to book it ASAP since booking will be out of books'\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ps.stem(w)}')    \n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "for w in words:\n",
    "    print(f'Word  {w}    has setmming      {ls.stem(w)}')    \n",
    "\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\n",
    "             \"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,ps.stem(word),ls.stem(word)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc1 = nlp(u\"I am a runner running in a race because I love to run since I ran yesterday\")\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)\n",
    "\n",
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')\n",
    "\n",
    "doc2 = nlp(u\"I saw eighteen mice today!\")\n",
    "\n",
    "show_lemmas(doc2)\n",
    "\n",
    "doc3 = nlp(u\"I am meeting him right now at the meeting.\")\n",
    "\n",
    "show_lemmas(doc3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"cats\",\"cacti\",\"radii\",\"feet\",\"speech\",'runner']\n",
    "\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word))\n",
    "\n",
    "\n",
    "print(lemmatizer.lemmatize(\"meeting\", \"n\"))\n",
    "print(lemmatizer.lemmatize(\"meeting\",'v'))\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))\n",
    "\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))\n",
    "\n",
    "words = [\"is\",\"was\",\"be\",\"been\",\"are\",\"were\"]\n",
    "\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word))\n",
    "\n",
    "words = [\"is\",\"was\",\"be\",\"been\",\"are\",\"were\"]\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word,'v'))\n",
    "    \n",
    "\n",
    "words = [\"feet\",\"radii\",\"men\",\"children\",\"carpenter\",\"fighter\"]\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word,'n'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))\n",
    "    \n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='arabic')\n",
    "\n",
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))\n",
    "\n",
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer , LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ls =  LancasterStemmer()\n",
    "\n",
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "for w in words:\n",
    "    print(ls.stem(w))\n",
    "\n",
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in words:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,ps.stem(word),ls.stem(word)))\n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
