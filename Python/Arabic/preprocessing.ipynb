{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "%pip install requests_html\n",
    "import requests\n",
    "# !pip install requests\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "def get_source(url):\n",
    "    \"\"\"Return the source code for the provided URL. \n",
    "    Args: \n",
    "        url (string): URL of the page to scrape.\n",
    "    Returns:\n",
    "        response (object): HTTP response object from requests_html. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(url)\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_google(query):\n",
    "    \n",
    "    # query = urllib.parse.quote_plus(query)\n",
    "    response = get_source(\"https://www.google.com/search?q=\" + query)\n",
    "    links = list(response.html.absolute_links)\n",
    "    google_domains = ('https://www.google.', \n",
    "                      'https://google.', \n",
    "                      'https://webcache.googleusercontent.', \n",
    "                      'http://webcache.googleusercontent.', \n",
    "                      'https://policies.google.',\n",
    "                      'https://support.google.',\n",
    "                      'https://maps.google.')\n",
    "\n",
    "    for url in links[:]:\n",
    "        if url.startswith(google_domains):\n",
    "            links.remove(url)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = scrape_google(\"سيارات\")\n",
    "for link in links:\n",
    "  print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import  word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# # EXAMPLE_TEXT = \"\"\"\n",
    "# # أبو عبد الله محمد بن موسى الخوارزمي عالم رياضيات وفلك\n",
    "# # وجغرافيا مسلم. يكنى باسم الخوارزمي وأبي جعفر. قيل أنه ولد حوالي 164هـ 781م (وهو غير مؤكد) وقيل أنه توفي بعد 232هـ أي (بعد 847م). يعتبر\n",
    "# # من أوائل علماء الرياضيات المسلمين حيث ساهمت أعماله بدور كبير في تقدم الرياضيات في عصره. اتصل بالخليفة العباسي المأمون وعمل في بيت الحكمة في \n",
    "# # بغداد وكسب ثقة الخليفة إذ ولاه المأمون بيت الحكمة كما عهد إليه برسم خارطة للأرض عمل فيها أكثر من سبعين جغرافيا. قبل وفاته في 850 م/232 هـ\n",
    "# # كان الخوارزمي قد ترك العديد من المؤلفات في علوم الرياضيات والفلك والجغرافيا ومن أهمها\n",
    "\n",
    "# # \"\"\"\n",
    "# EXAMPLE_TEXT=links\n",
    "# print(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links=scrape_google('cars')\n",
    "import numpy as np\n",
    "from fake_useragent import UserAgent\n",
    "import re\n",
    "from urllib.request import Request, urlopen\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_text(url):\n",
    "    try:\n",
    "        req = Request(url , headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urlopen(req,timeout=5).read()\n",
    "        soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "        texts = soup.findAll(text=True)\n",
    "        res=u\" \".join(t.strip() for t in texts if t.parent.name not in ['style', 'script', 'head', 'title', 'meta', '[document]'])\n",
    "        return(res)\n",
    "    except:\n",
    "        return False\n",
    "get_text('https://en.wikipedia.org/wiki/Machine_learning')\n",
    "text=[]\n",
    "for i in links:\n",
    "  t=get_text(i)\n",
    "  if t:\n",
    "    text.append(t)\n",
    "\n",
    "    word_tokens = word_tokenize(t)\n",
    "# word_tokens = \n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens]\n",
    "\n",
    "    # print(word_tokens)\n",
    "    # print('---------------------------------')\n",
    "    # print(filtered_sentence)\n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w in word_tokens]\n",
    "\n",
    "    # print(word_tokens)\n",
    "    # print('---------------------------------')\n",
    "    # print(filtered_sentence)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in word_tokens]\n",
    "\n",
    "    print(word_tokens)\n",
    "    print('---------------------------------')\n",
    "    print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import re\n",
    "st = ISRIStemmer()\n",
    "\n",
    "# w=filtered_sentence\n",
    "for word in word_tokens:\n",
    "    if word not  in stop_words:\n",
    "    #won't take the input correctly, input = output of tokenization# w= 'كلمات'\n",
    "        print(word+\"-->\"+st.stem(word))\n",
    "        u=st.stem(word)\n",
    "\n",
    "# import nltk\n",
    "\n",
    "# from nltk.stem.porter import *\n",
    "# p_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for word in  filtered_sentence:\n",
    "#     print(word+' --> '+p_stemmer.stem(word))\n",
    "\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "\n",
    "\n",
    "# for word in filtered_sentence:\n",
    "#     print(word+' --> '+s_stemmer.stem(word))\n",
    "# u=p_stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def tf_idf_analysis(keyword):\n",
    "    v = TfidfVectorizer(min_df=1,analyzer='word',ngram_range=(1,2),stop_words=stop_words)\n",
    "    x = v.fit_transform(text)\n",
    "\n",
    "    f = pd.DataFrame(x.toarray(), columns = v.get_feature_names())\n",
    "    d=pd.concat([pd.DataFrame(f.mean(axis=0)),pd.DataFrame(f.max(axis=0))],axis=1)\n",
    "    \n",
    "    \n",
    "    tf=pd.DataFrame((f>0).sum(axis=0))\n",
    "\n",
    "\n",
    "    d=d.reset_index().merge(tf.reset_index(),on='index',how='left')\n",
    "\n",
    "    d.columns=['word','average_tfidf','max_tfidf','frequency']\n",
    "\n",
    "    d['frequency']=round((d['frequency']/len(text))*100)\n",
    "\n",
    "    return(d)\n",
    "\n",
    "x= tf_idf_analysis(u)\n",
    "x[x['word'].str.isalpha()].sort_values('max_tfidf',ascending=False).head(35)\n",
    "# print(x)\n",
    "#######################################################################\n",
    "# import pandas as pd\n",
    "# # documentA = 'ذهب محمد الي الجامعة ليدرس الفيزياء و الكيمياء'\n",
    "# # documentB=u\n",
    "# documentA=u\n",
    "# bagOfWordsA = documentA.split(' ')\n",
    "# # bagOfWordsB = documentB.split(' ')\n",
    "# uniqueWords = set(bagOfWordsA)\n",
    "# uniqueWords\n",
    "# numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "\n",
    "# for word in bagOfWordsA:\n",
    "#     numOfWordsA[word] += 1\n",
    "# numOfWordsA\n",
    "# numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "\n",
    "# for word in bagOfWordsA:\n",
    "#     numOfWordsB[word] += 1\n",
    "\n",
    "# numOfWordsB    \n",
    "# def computeTF(wordDict, bagOfWords):\n",
    "#     tfDict = {}\n",
    "#     bagOfWordsCount = len(bagOfWords)\n",
    "#     for word, count in wordDict.items():\n",
    "#         tfDict[word] = count / float(bagOfWordsCount)\n",
    "#     return tfDict\n",
    "# tfB = computeTF(numOfWordsB, bagOfWordsA)\n",
    "# print(tfB)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
