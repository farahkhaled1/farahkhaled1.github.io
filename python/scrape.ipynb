{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hanien\\miniconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests_html in c:\\users\\hanien\\miniconda3\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from requests_html) (1.1.3)\n",
      "Requirement already satisfied: pyquery in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from requests_html) (2.0.0)\n",
      "Requirement already satisfied: parse in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from requests_html) (1.19.0)\n",
      "Requirement already satisfied: w3lib in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from requests_html) (2.1.1)\n",
      "Requirement already satisfied: requests in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from requests_html) (2.24.0)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from requests_html) (1.0.2)\n",
      "Requirement already satisfied: bs4 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from requests_html) (0.0.1)\n",
      "Requirement already satisfied: importlib-resources>=5.0; python_version < \"3.10\" in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from fake-useragent->requests_html) (5.12.0)\n",
      "Requirement already satisfied: lxml>=2.1 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from pyquery->requests_html) (4.9.2)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from pyquery->requests_html) (1.2.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from requests->requests_html) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from requests->requests_html) (2023.5.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from requests->requests_html) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from requests->requests_html) (2.10)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html) (10.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html) (6.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html) (4.51.0)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n",
      "Requirement already satisfied: pyee<9.0.0,>=8.1.0 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html) (8.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from bs4->requests_html) (4.12.2)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from importlib-resources>=5.0; python_version < \"3.10\"->fake-useragent->requests_html) (3.15.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from beautifulsoup4->bs4->requests_html) (2.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hanien\\miniconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in c:\\users\\hanien\\miniconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: click in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mysql-connector-python in c:\\users\\hanien\\miniconda3\\lib\\site-packages (8.0.33)\n",
      "Requirement already satisfied: protobuf<=3.20.3,>=3.11.0 in c:\\users\\hanien\\miniconda3\\lib\\site-packages (from mysql-connector-python) (3.20.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "https://choconutsworld.com/\n",
      "4\n",
      "['home', '-', 'choconuts', 'choconuts', 'chocolate', 'shop', 'welcome', 'choconuts', ',', 'handcraft', 'delicious', 'chocolates', 'using', 'finest', 'ingredients', '.', 'chocolates', 'made', 'fresh', 'daily', ',', 'focus', 'goodness', 'real', 'food', 'scratch-made', 'recipes', '.', 'specialize', 'chocolates', 'include', 'nuts', ',', 'creating', 'unique', 'flavor', 'combinations', 'sure', 'delight', 'taste', 'buds', '.', 'classic', 'nutty', 'chocolates', 'creative', ',', 'experimental', 'flavors', ',', 'something', 'every', 'chocolate', 'lover', '.', 'whether', 'celebrating', 'special', 'occasion', 'want', 'indulge', 'sweet', 'treat', ',', 'choconuts', 'covered', '.', 'we', \"'re\", 'passionate', 'providing', 'customers', 'one-of-a-kind', 'chocolate', 'experience', ',', 'ca', \"n't\", 'wait', 'share', 'you', '.', 'chocolates', 'classic', 'nutty', 'chocolates', 'creative', ',', 'experimental', 'flavors', ',', 'something', 'every', 'chocolate', 'lover', '.', 'also', 'offer', 'seasonal', 'holiday', 'chocolates', ',', 'well', 'gift', 'boxes', 'special', 'packaging', 'options', '.', 'take', 'pride', 'chocolate-making', 'craft', ',', 'we', \"'re\", 'dedicated', 'providing', 'customers', 'highest', 'quality', 'chocolates', '.', 'chocolates', 'made', 'fresh', 'daily', ',', 'strive', 'provide', 'customers', 'chocolate', 'experience', 'truly', 'unforgettable', '.', 'visit', 'online', 'shop', 'looking', 'order', 'delicious', 'choconuts', 'chocolates', '?', 'place', 'order', 'directly', 'instagram', 'page', '!', 'simply', 'send', 'us', 'direct', 'message', 'order', ',', 'we', \"'ll\", 'get', 'back', 'soon', 'possible', 'confirm', 'purchase.we', \"'re\", 'committed', 'providing', 'customers', 'convenient', 'hassle-free', 'shopping', 'experience', ',', 'we', \"'re\", 'always', 'available', 'answer', 'questions', 'may', 'have', '.', 'wait', '?', 'head', 'instagram', 'page', 'place', 'order', 'experience', 'deliciousness', 'choconuts', 'chocolates', '!', 'shop', 'take', 'great', 'care', 'packaging', 'chocolates', 'shipping', ',', 'use', 'insulated', 'packaging', 'ensure', 'arrive', 'best', 'possible', 'condition', '.', 'please', 'note', 'shipping', 'times', 'may', 'vary', 'due', 'weather-related', 'delays', 'factors', 'outside', 'control', '.', 'choconuts', ',', 'offer', 'wide', 'range', 'delicious', 'chocolates', ',', 'including', 'variety', 'flavors', 'packaging', 'options', '.', 'chocolates', 'made', 'using', 'finest', 'ingredients', ',', 'we', \"'re\", 'committed', 'creating', 'unique', 'innovative', 'flavor', 'combinations', '.', 'contact', 'hello', '@', 'choconuts.com', '123-456-789', 'follow', 'us', 'instagram', 'facebook', 'stay', 'up-to-date', 'latest', 'products', 'promotions', '.', 'love', 'share', 'photos', 'chocolates', ',', 'well', 'behind-the-scenes', 'glimpses', 'chocolate-making', 'process', '.', 'forget', 'tag', 'us', 'photos', 'using', '#', 'choconuts', '!', 'contact', 'us', 'free', 'wordpress', 'themes', 'created', 'wordpress', 'website', 'builder', '.']\n",
      "Data has been saved to the database.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install requests_html\n",
    "%pip install scikit-learn\n",
    "%pip install nltk\n",
    "%pip install mysql-connector-python\n",
    "\n",
    "import requests\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession\n",
    "import numpy as np\n",
    "from fake_useragent import UserAgent\n",
    "from urllib.request import Request, urlopen\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.data import url2pathname\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import mysql.connector\n",
    "# # Connect to the database\n",
    "# mysql = mysql.connector.connect(\n",
    "#     host=\"localhost\",\n",
    "#     user=\"root\",\n",
    "#     password=\"\",\n",
    "#     database=\"seopro\"\n",
    "# )\n",
    "# # Create a cursor object\n",
    "# mycursor = mysql.cursor()\n",
    "# # Execute an SQL query to select the last row from the table\n",
    "# mycursor.execute(\"SELECT uid,url FROM given_url ORDER BY id DESC LIMIT 1\")\n",
    "# # Fetch the last row from the table\n",
    "# url_arr = mycursor.fetchone()\n",
    "# # Extract the id and niche values from the row\n",
    "# uid = url_arr[0]\n",
    "# url = url_arr[1]\n",
    "# # Print the niche and id values\n",
    "# print(url)\n",
    "# print(uid)\n",
    "\n",
    "\n",
    "\n",
    "mysql = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"\",\n",
    "    database=\"seopro\"\n",
    ")\n",
    "# Create a cursor object\n",
    "mycursor = mysql.cursor()\n",
    "# Execute an SQL query to select the last row from the table\n",
    "mycursor.execute(\"SELECT uid,url FROM given_url ORDER BY id DESC LIMIT 1\")\n",
    "# Fetch the last row from the table\n",
    "url_arr = mycursor.fetchone()\n",
    "# Extract the id and niche values from the row\n",
    "uid = url_arr[0]\n",
    "url = url_arr[1]\n",
    "# Print the niche and id values\n",
    "print(url)\n",
    "print(uid)\n",
    "\n",
    "def get_text_from_url(url, words_per_line=5):\n",
    "    # Send a GET request to the URL and retrieve its HTML content\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # Extract the text content you need from the parsed HTML content\n",
    "    output = soup.get_text()\n",
    "    # Remove extra spaces and newlines from the text\n",
    "    output = ' '.join(output.split())\n",
    "\n",
    "    # Insert a newline after a certain number of words\n",
    "    words = output.split()\n",
    "    output = ''\n",
    "    for i in range(len(words)):\n",
    "        output += words[i] + ' '\n",
    "        if (i + 1) % words_per_line == 0:\n",
    "            output += '\\n'\n",
    "\n",
    "    return output\n",
    "\n",
    "# Call the get_text_from_url function\n",
    "# url = \"https://choconutsworld.com/\"\n",
    "output = get_text_from_url(url, words_per_line=5)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in output.lower().split() if word not in stop_words]\n",
    "    \n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(' '.join(filtered_words))\n",
    "print(words)\n",
    "    # Stem the words using Porter Stemmer\n",
    "    # stemmer = PorterStemmer()\n",
    "    # stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # # Join the stemmed words into a string\n",
    "    # stemmed_text = ' '.join(stemmed_words)\n",
    "    \n",
    "    # # Return the preprocessed text\n",
    "    # return stemmed_text\n",
    "\n",
    "# print (stemmed_text)\n",
    "\n",
    "\n",
    "\n",
    "# print(stemmed_words)\n",
    "# print(output)\n",
    "# Call the get_text_from_url function\n",
    "\n",
    "# sql = \"INSERT INTO synonyms (words,url, uid) VALUES (%s, %s, %s)\"\n",
    "# # for row in output_df:\n",
    "# val = (output_df, url, uid)\n",
    "# mycursor.execute(sql, val)\n",
    "# mysql.commit()\n",
    "# # Print a message to confirm that the data has been saved\n",
    "# print(\"Data has been saved to the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hanien\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 100.0% 1662.8/1662.8MB downloaded"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "# Download the pre-trained Word2Vec model\n",
    "model = api.load('word2vec-google-news-300')\n",
    "# import nltk\n",
    "# import gensim.downloader as api\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "# # Download the pre-trained Word2Vec model\n",
    "# model = api.load('word2vec-google-news-300')\n",
    "\n",
    "word_list = words\n",
    "\n",
    "# # Load stop words and create a lemmatizer object\n",
    "# stop_words = stopwords.words('english')\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# # Define a function to preprocess text\n",
    "# def preprocess(text):\n",
    "#     tokens = nltk.word_tokenize(text.lower())\n",
    "#     tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "#     tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "#     return tokens\n",
    "\n",
    "# # Preprocess the words in the list\n",
    "# word_list_preprocessed = [preprocess(word) for word in word_list]\n",
    "\n",
    "# Define a function to find the alternative words for a word with high Word2Vec similarity scores\n",
    "def find_high_similarity_alternative_words(word_list, threshold=0.6):\n",
    "    alternatives = []\n",
    "    if word not in model:\n",
    "        return alternatives\n",
    "    for w, score in model.most_similar(word):\n",
    "        if score > threshold:\n",
    "            alternatives.append(w)\n",
    "    return alternatives\n",
    "\n",
    "# Find the higher ranking alternative words for each word in the list\n",
    "alternatives_dict = {}\n",
    "for word in word_list:\n",
    "    alternatives = find_high_similarity_alternative_words(word)\n",
    "    alternatives_dict[word] = alternatives\n",
    "\n",
    "# Print the higher ranking alternative words for each word in the list\n",
    "for word, alternatives in alternatives_dict.items():\n",
    "    print(word + ':', alternatives)\n",
    "    sql = \"INSERT INTO synonyms (words_before,words_after,url, uid) VALUES (%s,%s, %s, %s)\"\n",
    "# for row in output_df:\n",
    "    val = (word,str(alternatives), url, uid)\n",
    "    mycursor.execute(sql, val)\n",
    "    mysql.commit()\n",
    "# Print a message to confirm that the data has been saved\n",
    "print(\"Data has been saved to the database.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
