{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests_html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\xampp\\htdocs\\seopro\\Python\\preprocess-tfidg.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/seopro/Python/preprocess-tfidg.ipynb#ch0000000?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/seopro/Python/preprocess-tfidg.ipynb#ch0000000?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/seopro/Python/preprocess-tfidg.ipynb#ch0000000?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrequests_html\u001b[39;00m \u001b[39mimport\u001b[39;00m HTML\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/seopro/Python/preprocess-tfidg.ipynb#ch0000000?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrequests_html\u001b[39;00m \u001b[39mimport\u001b[39;00m HTMLSession\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/seopro/Python/preprocess-tfidg.ipynb#ch0000000?line=10'>11</a>\u001b[0m \u001b[39m# new_stopwords = [\"all\", \"due\", \"to\", \"on\", \"daily\",\"per\",\"article\",\"com\",\"http\"]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/seopro/Python/preprocess-tfidg.ipynb#ch0000000?line=11'>12</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/seopro/Python/preprocess-tfidg.ipynb#ch0000000?line=12'>13</a>\u001b[0m \u001b[39m# stop_words = nltk.corpus.stopwords.words('english')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/seopro/Python/preprocess-tfidg.ipynb#ch0000000?line=31'>32</a>\u001b[0m \u001b[39m# stopWords.extend(new_stopwords)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/seopro/Python/preprocess-tfidg.ipynb#ch0000000?line=32'>33</a>\u001b[0m \u001b[39m# print(stopWords)\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests_html'"
     ]
    }
   ],
   "source": [
    "# pip install fake-useragent\n",
    "\n",
    "# pip install requests-html \n",
    "\n",
    "import requests\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "# new_stopwords = [\"all\", \"due\", \"to\", \"on\", \"daily\",\"per\",\"article\",\"com\",\"http\"]\n",
    "\n",
    "# stop_words = nltk.corpus.stopwords.words('english')\n",
    "# stop_words.extend(new_stopwords)\n",
    "# print(stop_words)\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# # # stpwrd = nltk.corpus.stopwords.words('english')\n",
    "# # # new_stopwords = [\"http\",\"com\"]\n",
    "# # # stpwrd.extend(new_stopwords)\n",
    "# # # print(stpwrd)\n",
    "\n",
    "# # stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "# # stop_words = list(set(stpwrd))\n",
    "# # print(stop_words)\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# stopWords = list(set(stopwords.words('english')))\n",
    "# new_stopwords = [\"http\",\"com\",\"per\",\"www\",\"min\",\"max\",\"edu\",\"https\",\"blog\",\"start\",\"end\",\"google\",\"ar\",\"en\",\"said\",\"type\",\"div\",\"see\",\"seen\",\"say\",\"said\"]\n",
    "# stopWords.extend(new_stopwords)\n",
    "# print(stopWords)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from fake_useragent import UserAgent\n",
    "import re\n",
    "from urllib.request import Request, urlopen\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_text(url):\n",
    "    try:\n",
    "        req = Request(url , headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urlopen(req,timeout=5).read()\n",
    "        soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "        texts = soup.findAll(text=True)\n",
    "        res=u\" \".join(t.strip() for t in texts if t.parent.name not in ['style', 'script', 'head', 'title', 'meta', '[document]'])\n",
    "        return(res)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "get_text('https://en.wikipedia.org/wiki/Machine_learning')\n",
    "\n",
    "# # get_text('https://en.wikipedia.org/wiki/Machine_learning')[0:6000]\n",
    "# # get_text('https://en.wikipedia.org/wiki/Machine_learning')\n",
    "\n",
    "# #I will return the first 500 characters\\\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# print(len(stop_words))\n",
    "# stop_words\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# # example_sent = \"This is a sample sentence, And This showing off the stop words filtration.\"\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# word_tokens = word_tokenize(get_text('https://en.wikipedia.org/wiki/Machine_learning'))\n",
    "# word_tokens\n",
    "\n",
    "# filtered_sentence = [w for w in word_tokens]\n",
    "\n",
    "# print(word_tokens)\n",
    "# print('---------------------------------')\n",
    "# print(filtered_sentence)\n",
    "\n",
    "# filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "# print(word_tokens)\n",
    "# print('---------------------------------')\n",
    "# print(filtered_sentence)\n",
    "\n",
    "# filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "# print(word_tokens)\n",
    "# print('---------------------------------')\n",
    "# print(filtered_sentence)\n",
    "\n",
    "\n",
    "# from nltk.data import url2pathname\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# print(len(stop_words))\n",
    "# stop_words\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# # example_sent = get_text('https://en.wikipedia.org/wiki/Machine_learning')\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# links=scrape_google('tote bags')\n",
    "\n",
    "# text=[]\n",
    "# for i in links:\n",
    "#   t=get_text(i)\n",
    "#   if t:\n",
    "#     text.append(t)\n",
    "\n",
    "#     word_tokens = word_tokenize(t)\n",
    "# # word_tokens = \n",
    "\n",
    "#     # filtered_sentence = [w for w in word_tokens]\n",
    "\n",
    "#     # print(word_tokens)\n",
    "#     # print('---------------------------------')\n",
    "#     # print(filtered_sentence)\n",
    "\n",
    "#     # filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "#     # print(word_tokens)\n",
    "#  # print('---------------------------------')\n",
    "#     # print(filtered_sentence)\n",
    "#     filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "#     print(word_tokens)\n",
    "#     print('---------------------------------')\n",
    "#     print(filtered_sentence)\n",
    "\n",
    "# import nltk\n",
    "\n",
    "# from nltk.stem.porter import *\n",
    "# p_stemmer = PorterStemmer()\n",
    "\n",
    "# words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "# for word in words:\n",
    "#     print(word+' --> '+p_stemmer.stem(word))\n",
    "\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "# for word in words:\n",
    "#     print(word+' --> '+s_stemmer.stem(word))\n",
    "\n",
    "# stemming\n",
    "\n",
    "# import these modules\n",
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "  \n",
    "# ps = PorterStemmer()\n",
    "  \n",
    "# # choose some words to be stemmed\n",
    "# #words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
    "# words = get_text('https://google.com')\n",
    "\n",
    "# for w in words:\n",
    "#     print(words,':', ps.stem(words))\n",
    "\n",
    "##############################################\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Get the URLs from competitors\n",
    "\n",
    "\n",
    "# def google_results(keyword, n_results):\n",
    "#     query = keyword\n",
    "#     query = urllib.parse.quote_plus(query) # Format into URL encoding\n",
    "#     number_result = n_results\n",
    "#     ua = UserAgent()\n",
    "#     google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(number_result)\n",
    "#     response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
    "#     soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#     result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})\n",
    "#     results=[re.search('\\/url\\?q\\=(.*)\\&sa',str(i.find('a', href = True)['href'])) for i in result_div if \"url\" in str(i)]\n",
    "#     links=[i.group(1) for i in results if i != None]\n",
    "#     return (soup)\n",
    "\n",
    "# google_results('machine learning',10)\n",
    "\n",
    "def get_source(url):\n",
    "    \"\"\"Return the source code for the provided URL. \n",
    "\n",
    "    Args: \n",
    "        url (string): URL of the page to scrape.\n",
    "\n",
    "    Returns:\n",
    "        response (object): HTTP response object from requests_html. \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(url)\n",
    "        return response\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def scrape_google(query):\n",
    "\n",
    "    query = urllib.parse.quote_plus(query)\n",
    "    response = get_source(\"https://www.google.co.uk/search?q=\" + query)\n",
    "\n",
    "    links = list(response.html.absolute_links)\n",
    "    google_domains = ('https://www.google.', \n",
    "                      'https://google.', \n",
    "                      'https://webcache.googleusercontent.', \n",
    "                      'http://webcache.googleusercontent.', \n",
    "                      'https://policies.google.',\n",
    "                      'https://support.google.',\n",
    "                      'https://maps.google.')\n",
    "\n",
    "    for url in links[:]:\n",
    "        if url.startswith(google_domains):\n",
    "            links.remove(url)\n",
    "\n",
    "    return links\n",
    "\n",
    "scrape_google(\"cars\")\n",
    "\n",
    "# Step 3: Analyse the text and get the most important words.\n",
    "\n",
    "def tf_idf_analysis(keyword):\n",
    "    # links=google_results(keyword,12)\n",
    "    links=scrape_google(keyword)\n",
    "\n",
    "    text=[]\n",
    "    for i in links:\n",
    "        t=get_text(i)\n",
    "        if t:\n",
    "            text.append(t)\n",
    "            \n",
    "#     v = TfidfVectorizer(min_df=2,analyzer='word',ngram_range=(1,2),stop_words=stopWords)\n",
    "#     x = v.fit_transform(text)\n",
    "\n",
    "#     f = pd.DataFrame(x.toarray(), columns = v.get_feature_names())\n",
    "#     d=pd.concat([pd.DataFrame(f.mean(axis=0)),pd.DataFrame(f.max(axis=0))],axis=1)\n",
    "    \n",
    "    \n",
    "#     tf=pd.DataFrame((f>0).sum(axis=0))\n",
    "\n",
    "\n",
    "#     d=d.reset_index().merge(tf.reset_index(),on='index',how='left')\n",
    "\n",
    "#     d.columns=['word','average_tfidf','max_tfidf','frequency']\n",
    "\n",
    "\n",
    "# #you can comment the following part if you want the number of URLs that the word occurs. The percentage makes sense\n",
    "# #when we have a lot of URLs to check\n",
    "\n",
    "#     d['frequency']=round((d['frequency']/len(text))*100)\n",
    "\n",
    "#     return(d)\n",
    "\n",
    "\n",
    "# x= tf_idf_analysis('tote bags')\n",
    "# x[x['word'].str.isalpha()].sort_values('max_tfidf',ascending=False).head(35)\n",
    "\n",
    "from nltk.data import url2pathname\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(len(stop_words))\n",
    "stop_words\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# example_sent = get_text('https://en.wikipedia.org/wiki/Machine_learning')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "links=scrape_google('cars')\n",
    "\n",
    "text=[]\n",
    "for i in links:\n",
    "  t=get_text(i)\n",
    "  if t:\n",
    "    text.append(t)\n",
    "\n",
    "    word_tokens = word_tokenize(t)\n",
    "# word_tokens = \n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens]\n",
    "\n",
    "    print(word_tokens)\n",
    "    print('---------------------------------')\n",
    "    print(filtered_sentence)\n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    print(word_tokens)\n",
    "    print('---------------------------------')\n",
    "    print(filtered_sentence)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "    print(word_tokens)\n",
    "    print('---------------------------------')\n",
    "    print(filtered_sentence)\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "# links=scrape_google('tote bags')\n",
    "\n",
    "# text=[]\n",
    "# for i in links:\n",
    "#   t=get_text(i)\n",
    "#   if t:\n",
    "#     text.append(t)\n",
    "for word in  filtered_sentence:\n",
    "    print(word+' --> '+p_stemmer.stem(word))\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "for word in filtered_sentence:\n",
    "    print(word+' --> '+s_stemmer.stem(word))\n",
    "u=s_stemmer.stem(word)\n",
    "\n",
    "def tf_idf_analysis(keyword):\n",
    "  v = TfidfVectorizer(min_df=1,analyzer='word',ngram_range=(1,2),stop_words=stop_words)\n",
    "  x = v.fit_transform(text)\n",
    "\n",
    "  f = pd.DataFrame(x.toarray(), columns = v.get_feature_names())\n",
    "  d=pd.concat([pd.DataFrame(f.mean(axis=0)),pd.DataFrame(f.max(axis=0))],axis=1)\n",
    "    \n",
    "    \n",
    "  tf=pd.DataFrame((f>0).sum(axis=0))\n",
    "\n",
    "\n",
    "  d=d.reset_index().merge(tf.reset_index(),on='index',how='left')\n",
    "\n",
    "  d.columns=['word','average_tfidf','max_tfidf','frequency']\n",
    "\n",
    "\n",
    "#you can comment the following part if you want the number of URLs that the word occurs. The percentage makes sense\n",
    "#when we have a lot of URLs to check\n",
    "\n",
    "  d['frequency']=round((d['frequency']/len(text))*100)\n",
    "\n",
    "  return(d)\n",
    "\n",
    "\n",
    "x= tf_idf_analysis('cars')\n",
    "x[x['word'].str.isalpha()].sort_values('max_tfidf',ascending=False).head(35)\n",
    "\n",
    "# test test tf-idf\n",
    "\n",
    "def tf_idf_analysis(keyword):\n",
    "    # links=google_results(keyword,12)\n",
    "    links=scrape_google(keyword)\n",
    "\n",
    "    text=[]\n",
    "    for i in links:\n",
    "        t=get_text(i)\n",
    "        if t:\n",
    "            text.append(t)\n",
    "            \n",
    "    v = TfidfVectorizer(min_df=2,analyzer='word',ngram_range=(1,2),stop_words=stopWords)\n",
    "    x = v.fit_transform(text)\n",
    "\n",
    "    f = pd.DataFrame(x.toarray(), columns = v.get_feature_names_out())\n",
    "    d=pd.concat([pd.DataFrame(f.mean(axis=0)),pd.DataFrame(f.max(axis=0))],axis=1)\n",
    "    \n",
    "    \n",
    "    tf=pd.DataFrame((f>0).sum(axis=0))\n",
    "\n",
    "\n",
    "    d=d.reset_index().merge(tf.reset_index(),on='index',how='left')\n",
    "\n",
    "    d.columns=['word','average_tfidf','max_tfidf','frequency']\n",
    "\n",
    "\n",
    "#you can comment the following part if you want the number of URLs that the word occurs. The percentage makes sense\n",
    "#when we have a lot of URLs to check\n",
    "\n",
    "    d['frequency']=round((d['frequency']/len(text))*100)\n",
    "\n",
    "    return(v.get_feature_names())\n",
    "\n",
    "\n",
    "x= tf_idf_analysis('tote bags')\n",
    "print(x)\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "ps = PorterStemmer()\n",
    "  \n",
    "# choose some words to be stemmed\n",
    "#words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
    "words = get_text('https://google.com')\n",
    "\n",
    "for w in words:\n",
    "    print(words,':', ps.stem(words))\n",
    "\n",
    "# -----------------\n",
    "\n",
    "\n",
    "\n",
    "# x= tf_idf_analysis('tote bags')\n",
    "\n",
    "# #remove the numbers and sort by max tfidf and get the top20 words\n",
    "# #x['word'].str.isalpha().head(35)\n",
    "# x[x['word'].str.isalpha()].sort_values('max_tfidf',ascending=False).head(35)\n",
    "\n",
    "\n",
    "\n",
    "# # import required modules\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # taking input\n",
    "# text = \"run writing\"\n",
    "\n",
    "# # returns a document of object\n",
    "# doc = nlp(text)\n",
    "\n",
    "# # checking if it is a noun or not\n",
    "# if(doc[0].tag_ == 'NNP'):\n",
    "# \tprint(text, \" is a noun.\")\n",
    "# else:\n",
    "# \tprint(text, \" is a verb.\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2ccb58c476f33ba3e3aee7ac07234ef6b8217ef24ad64d2a7d4fed1a57c1cd2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
