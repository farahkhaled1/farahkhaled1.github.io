{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pllot\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_12020\\2751849280.py:69: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  texts = soup.findAll(text=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              word  average_tfidf  max_tfidf  frequency\n",
      "8729       privacy          0.004      0.015       70.0\n",
      "9584        search          0.010      0.041       65.0\n",
      "11623          use          0.012      0.060       61.0\n",
      "1617          best          0.006      0.028       61.0\n",
      "7347           new          0.052      0.261       57.0\n",
      "2474         class          0.018      0.115       57.0\n",
      "2395        choose          0.011      0.057       57.0\n",
      "5252          help          0.006      0.020       52.0\n",
      "7599           one          0.021      0.118       52.0\n",
      "4921           get          0.012      0.091       52.0\n",
      "9646           see          0.023      0.146       52.0\n",
      "2786       contact          0.005      0.020       52.0\n",
      "8104         pilot          0.107      0.381       52.0\n",
      "8537        policy          0.004      0.018       52.0\n",
      "5844   information          0.006      0.027       52.0\n",
      "4665        follow          0.004      0.011       48.0\n",
      "3824           end          0.012      0.170       48.0\n",
      "7630        online          0.005      0.017       48.0\n",
      "11108         time          0.007      0.041       48.0\n",
      "6996          menu          0.005      0.017       48.0\n",
      "10979        terms          0.005      0.021       48.0\n",
      "1973        button          0.013      0.160       48.0\n",
      "3866       english          0.040      0.417       43.0\n",
      "2806       content          0.009      0.086       43.0\n",
      "7401          next          0.009      0.035       43.0\n",
      "4465          find          0.005      0.017       43.0\n",
      "7310          need          0.007      0.045       43.0\n",
      "6922           may          0.011      0.069       43.0\n",
      "12050      whether          0.006      0.029       43.0\n",
      "2547            co          0.008      0.043       43.0\n",
      "9799       service          0.003      0.016       43.0\n",
      "11484          two          0.005      0.021       43.0\n",
      "11173          top          0.006      0.029       43.0\n",
      "11972          way          0.004      0.017       39.0\n",
      "1302        around          0.002      0.011       39.0\n",
      "Data has been saved to the database.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession\n",
    "import numpy as np\n",
    "from fake_useragent import UserAgent\n",
    "from urllib.request import Request, urlopen\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.data import url2pathname\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words.update(['le', 'eu', 'span', 'ago', 'pp', 'ue', 'div', 'src', 'page', 'egp', 'url', 'cdn', 'alt', 'com', 'net', 'org', 'cdn', 'img', 'google','eg','usd','http','https','net','us','eg'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import mysql.connector\n",
    "\n",
    "# Connect to the database\n",
    "mysql = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"\",\n",
    "    database=\"seopro\"\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "mycursor = mysql.cursor()\n",
    "\n",
    "# Execute an SQL query to select the last row from the table\n",
    "mycursor.execute(\"SELECT uid, niche FROM given_niche ORDER BY id DESC LIMIT 1\")\n",
    "\n",
    "# Fetch the last row from the table\n",
    "niche_arr = mycursor.fetchone()\n",
    "\n",
    "# # Convert niche[0] to a string\n",
    "# niche= str(niche_arr[0])\n",
    "\n",
    "# # Print the last row as a string\n",
    "# print(niche)\n",
    "\n",
    "# Extract the id and niche values from the row\n",
    "uid = niche_arr[0]\n",
    "niche = niche_arr[1]\n",
    "\n",
    "# Print the niche and id values\n",
    "print(niche)\n",
    "print(uid)\n",
    "\n",
    "\n",
    "def get_text(url):\n",
    "    try:\n",
    "        req = Request(url , headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urlopen(req,timeout=5).read()\n",
    "        soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "        texts = soup.findAll(text=True)\n",
    "        res=u\" \".join(t.strip() for t in texts if t.parent.name not in ['style', 'script', 'head', 'title', 'meta', '[document]'])\n",
    "        return(res)\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "\"\"\"Step 2: Get the URLs from competitors\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_source(url):\n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(url)\n",
    "\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "def scrape_google(query):\n",
    "\n",
    "    query = urllib.parse.quote_plus(query)\n",
    "    response = get_source(\"https://www.google.com/search?q=\" + query)\n",
    "    links = list(response.html.absolute_links)\n",
    "    # google_domains = ('https://www.google.', \n",
    "    #                   'https://google.', \n",
    "    #                   'https://webcache.googleusercontent.', \n",
    "    #                   'http://webcache.googleusercontent.', \n",
    "    #                   'https://policies.google.',\n",
    "    #                   'https://support.google.',\n",
    "    #                   'https://maps.google.')\n",
    "    \n",
    "    google_domains = ('https://www.google.', \n",
    "                      'https://google.')\n",
    "     \n",
    "\n",
    "    for url in links[:]:\n",
    "        if url.startswith(google_domains):\n",
    "            links.remove(url)\n",
    "\n",
    "    return links\n",
    "\n",
    "# niche = input(\"enter your niche\")\n",
    "# scrape_google(niche)\n",
    "\n",
    "\"\"\"Step 3: Analyse the text and get the most important words.\"\"\"\n",
    "\n",
    "# niche = input(\"enter your niche: \")\n",
    "links=scrape_google(niche)\n",
    "\n",
    "text=[]\n",
    "for i in links:\n",
    "  t=get_text(i)\n",
    "  if t:\n",
    "    text.append(t)\n",
    "\n",
    "    word_tokens = word_tokenize(t)\n",
    "\n",
    "   \n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    \n",
    "\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for word in filtered_sentence:\n",
    "    # if len(word) >= 3:\n",
    "    word+' --> '+p_stemmer.stem(word)\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "\n",
    "\n",
    "for word in filtered_sentence:\n",
    "    if len(word) >=4:\n",
    "        word+' --> '+s_stemmer.stem(word)\n",
    "u=p_stemmer.stem(word)\n",
    "import mysql.connector\n",
    "\n",
    "########\n",
    "# Modify the tf_idf_analysis function to return a DataFrame\n",
    "def tf_idf_analysis(keyword):\n",
    "    v = TfidfVectorizer(min_df=1,analyzer='word',ngram_range=(1,2),stop_words=list(stop_words))\n",
    "    x = v.fit_transform(text)\n",
    "    f = pd.DataFrame(x.toarray(), columns = v.get_feature_names_out())\n",
    "    d=pd.concat([pd.DataFrame(f.mean(axis=0)),pd.DataFrame(f.max(axis=0))],axis=1)\n",
    "    tf=pd.DataFrame((f>0).sum(axis=0))\n",
    "    d=d.reset_index().merge(tf.reset_index(),on='index',how='left')\n",
    "    d.columns=['word','average_tfidf','max_tfidf','frequency']\n",
    "    d['frequency']=round((d['frequency']/len(text))*100)\n",
    "    d['max_tfidf'] = d['max_tfidf'].round(3)\n",
    "    d['average_tfidf'] = d['average_tfidf'].round(3)\n",
    "\n",
    "    d= d[d['word'].str.isalpha()].sort_values('frequency',ascending=False).head(35)\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "# Call the tf_idf_analysis function and store the output in a DataFrame variable\n",
    "output_df = tf_idf_analysis(u)\n",
    "print (output_df)\n",
    "# Convert the DataFrame to a dictionary\n",
    "output_dict = output_df.to_dict('records')\n",
    "\n",
    "# Connect to the database\n",
    "mysql = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    passwd=\"\",\n",
    "    database=\"seopro\"\n",
    ")\n",
    "\n",
    "# Insert the data into the database\n",
    "mycursor = mysql.cursor()\n",
    "sql = \"INSERT INTO keywords (word, average_tfidf, max_tfidf, frequency, niche, uid) VALUES (%s, %s, %s, %s, %s, %s)\"\n",
    "for row in output_dict:\n",
    "    val = (row['word'], row['average_tfidf'], row['max_tfidf'], row['frequency'], niche, uid)\n",
    "    mycursor.execute(sql, val)\n",
    "mysql.commit()\n",
    "\n",
    "# Print a message to confirm that the data has been saved\n",
    "print(\"Data has been saved to the database.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8836dc3cb5d022b839c66561b07c94e2b96c073f18eae4c463d3a3bcf20aef43"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
