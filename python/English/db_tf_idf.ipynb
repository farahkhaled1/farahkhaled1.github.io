{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs\n",
      "9\n",
      "['https://maps.google.com/maps?output=search&q=dogs&entry=mc&sa=X&ved=2ahUKEwiIhZuH_cX_AhUcpVYBHbfcD_oQ0pQJegQIBBAB', 'https://translate.google.com/translate?hl=ar&sl=en&u=https://www.britannica.com/animal/dog&prev=search&pto=aue', 'https://www.youtube.com/watch?v=IACqfc8BtK4', 'https://en.wikipedia.org/wiki/Lists_of_dogs', 'https://translate.google.com/translate?hl=ar&sl=en&u=https://a-z-animals.com/pets/dogs/&prev=search&pto=aue', 'https://translate.google.com/translate?hl=ar&sl=en&u=https://www.cdc.gov/healthypets/pets/dogs.html&prev=search&pto=aue', 'https://www.goodhousekeeping.com/life/pets/g4531/cutest-dog-breeds/', 'https://en.wikipedia.org/wiki/Pleistocene_wolf', 'https://www.dogstrust.org.uk/', 'https://translate.google.com/translate?hl=ar&sl=en&u=https://en.wikipedia.org/wiki/Dog&prev=search&pto=aue', 'https://ar.wikipedia.org/wiki/%D9%83%D9%84%D8%A8', 'https://translate.google.com/translate?hl=ar&sl=en&u=https://www.rspca.org.uk/adviceandwelfare/pets/dogs&prev=search&pto=aue', 'https://policies.google.com/terms?hl=ar&fg=1', 'https://www.youtube.com/watch?v=5pLCmLgjiJ8', 'https://en.wikipedia.org/wiki/Cultural_depictions_of_dogs', 'https://www.cdc.gov/healthypets/pets/dogs.html', 'https://a-z-animals.com/pets/dogs/', 'http://en.wikipedia.org/wiki/Canis', 'https://www.nationalgeographic.com/animals/mammals/facts/domestic-dog', 'https://support.google.com/websearch/answer/181196?hl=ar', 'https://translate.google.com/translate?hl=ar&sl=en&u=https://www.goodhousekeeping.com/life/pets/g4531/cutest-dog-breeds/&prev=search&pto=aue', 'https://en.wikipedia.org/wiki/List_of_individual_dogs', 'https://translate.google.com/translate?hl=ar&sl=en&u=https://www.akc.org/dog-breeds/&prev=search&pto=aue', 'https://policies.google.com/privacy?hl=ar&fg=1', 'https://www.rspca.org.uk/adviceandwelfare/pets/dogs', 'https://translate.google.com/translate?hl=ar&sl=en&u=https://www.nationalgeographic.com/animals/mammals/facts/domestic-dog&prev=search&pto=aue', 'https://en.wikipedia.org/wiki/Dog', 'https://www.britannica.com/animal/dog', 'https://support.google.com/websearch/?p=ws_results_help&hl=ar&fg=1', 'https://www.youtube.com/watch?v=MPV2METPeJU', 'https://www.akc.org/dog-breeds/']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_5784\\118540272.py:70: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  texts = soup.findAll(text=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              word  average_tfidf  max_tfidf  frequency\n",
      "20082         dogs          0.140      0.498       74.0\n",
      "23640       family          0.011      0.033       74.0\n",
      "19214          dog          0.162      0.420       74.0\n",
      "49529       search          0.006      0.027       71.0\n",
      "22196      english          0.009      0.046       71.0\n",
      "12032       breeds          0.039      0.159       68.0\n",
      "16242      content          0.008      0.039       68.0\n",
      "7925        animal          0.016      0.090       68.0\n",
      "41168          one          0.018      0.061       68.0\n",
      "30927  information          0.008      0.052       65.0\n",
      "45281      privacy          0.006      0.036       65.0\n",
      "28712         home          0.010      0.087       65.0\n",
      "10215       become          0.006      0.038       61.0\n",
      "37438          may          0.017      0.081       61.0\n",
      "49637          see          0.010      0.035       61.0\n",
      "56846       united          0.005      0.012       61.0\n",
      "35658       living          0.005      0.027       61.0\n",
      "39698     national          0.007      0.031       61.0\n",
      "7579      american          0.016      0.095       61.0\n",
      "35143         like          0.015      0.065       61.0\n",
      "42806       people          0.027      0.257       61.0\n",
      "57070          use          0.013      0.118       61.0\n",
      "12010     breeding          0.004      0.012       58.0\n",
      "16169      contact          0.010      0.075       58.0\n",
      "59595         work          0.005      0.028       58.0\n",
      "56446          two          0.008      0.031       58.0\n",
      "56509         type          0.007      0.031       58.0\n",
      "28224         help          0.009      0.047       58.0\n",
      "28568      history          0.014      0.075       58.0\n",
      "60051        years          0.016      0.100       58.0\n",
      "54148         tail          0.004      0.014       58.0\n",
      "54695        terms          0.004      0.023       58.0\n",
      "29173        human          0.012      0.074       58.0\n",
      "11868        breed          0.025      0.132       58.0\n",
      "40118          new          0.010      0.036       58.0\n",
      "Data has been saved to the database.\n",
      "45.79236435890198\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession\n",
    "import numpy as np\n",
    "from fake_useragent import UserAgent\n",
    "from urllib.request import Request, urlopen\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.data import url2pathname\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words.update(['le', 'eu', 'span', 'ago', 'pp', 'ue', 'div', 'src', 'page', 'egp', 'url', 'cdn', 'alt', 'com', 'net', 'org', 'cdn', 'img', 'google','eg','usd','http','https','net','us','eg'])\n",
    "\n",
    "import time\n",
    "t0=time.time()\n",
    "\n",
    "\n",
    "import mysql.connector\n",
    "\n",
    "# Connect to the database\n",
    "mysql = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"\",\n",
    "    database=\"seopro\"\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "mycursor = mysql.cursor()\n",
    "\n",
    "# Execute an SQL query to select the last row from the table\n",
    "mycursor.execute(\"SELECT uid, niche FROM given_niche ORDER BY id DESC LIMIT 1\")\n",
    "\n",
    "# Fetch the last row from the table\n",
    "niche_arr = mycursor.fetchone()\n",
    "\n",
    "# # Convert niche[0] to a string\n",
    "# niche= str(niche_arr[0])\n",
    "\n",
    "# # Print the last row as a string\n",
    "# print(niche)\n",
    "\n",
    "# Extract the id and niche values from the row\n",
    "uid = niche_arr[0]\n",
    "niche = niche_arr[1]\n",
    "\n",
    "# Print the niche and id values\n",
    "print(niche)\n",
    "print(uid)\n",
    "\n",
    "\n",
    "def get_text(url):\n",
    "    try:\n",
    "        req = Request(url , headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urlopen(req,timeout=5).read()\n",
    "        soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "        texts = soup.findAll(text=True)\n",
    "        res=u\" \".join(t.strip() for t in texts if t.parent.name not in ['style', 'script', 'head', 'title', 'meta', '[document]'])\n",
    "        return(res)\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "\"\"\"Step 2: Get the URLs from competitors\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_source(url):\n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(url)\n",
    "\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "def scrape_google(query):\n",
    "\n",
    "    query = urllib.parse.quote_plus(query)\n",
    "    response = get_source(\"https://www.google.com/search?q=\" + query)\n",
    "    links = list(response.html.absolute_links)\n",
    "    # google_domains = ('https://www.google.', \n",
    "    #                   'https://google.', \n",
    "    #                   'https://webcache.googleusercontent.', \n",
    "    #                   'http://webcache.googleusercontent.', \n",
    "    #                   'https://policies.google.',\n",
    "    #                   'https://support.google.',\n",
    "    #                   'https://maps.google.')\n",
    "    \n",
    "    google_domains = ('https://www.google.', \n",
    "                      'https://google.')\n",
    "     \n",
    "\n",
    "    for url in links[:]:\n",
    "        if url.startswith(google_domains):\n",
    "            links.remove(url)\n",
    "\n",
    "    return links\n",
    "\n",
    "# niche = input(\"enter your niche\")\n",
    "# scrape_google(niche)\n",
    "\n",
    "\"\"\"Step 3: Analyse the text and get the most important words.\"\"\"\n",
    "\n",
    "# niche = input(\"enter your niche: \")\n",
    "links=scrape_google(niche)\n",
    "print(links)\n",
    "text=[]\n",
    "for i in links:\n",
    "  t=get_text(i)\n",
    "  if t:\n",
    "    text.append(t)\n",
    "\n",
    "    word_tokens = word_tokenize(t)\n",
    "\n",
    "   \n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    \n",
    "\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for word in filtered_sentence:\n",
    "    # if len(word) >= 3:\n",
    "    word+' --> '+p_stemmer.stem(word)\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "\n",
    "\n",
    "for word in filtered_sentence:\n",
    "    if len(word) >=4:\n",
    "        word+' --> '+s_stemmer.stem(word)\n",
    "u=p_stemmer.stem(word)\n",
    "import mysql.connector\n",
    "\n",
    "########\n",
    "# Modify the tf_idf_analysis function to return a DataFrame\n",
    "def tf_idf_analysis(keyword):\n",
    "    v = TfidfVectorizer(min_df=1,analyzer='word',ngram_range=(1,2),stop_words=list(stop_words))\n",
    "    x = v.fit_transform(text)\n",
    "    f = pd.DataFrame(x.toarray(), columns = v.get_feature_names_out())\n",
    "    d=pd.concat([pd.DataFrame(f.mean(axis=0)),pd.DataFrame(f.max(axis=0))],axis=1)\n",
    "    tf=pd.DataFrame((f>0).sum(axis=0))\n",
    "    d=d.reset_index().merge(tf.reset_index(),on='index',how='left')\n",
    "    d.columns=['word','average_tfidf','max_tfidf','frequency']\n",
    "    d['frequency']=round((d['frequency']/len(text))*100)\n",
    "    d['max_tfidf'] = d['max_tfidf'].round(3)\n",
    "    d['average_tfidf'] = d['average_tfidf'].round(3)\n",
    "\n",
    "    d= d[d['word'].str.isalpha()].sort_values('frequency',ascending=False).head(35)\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "# Call the tf_idf_analysis function and store the output in a DataFrame variable\n",
    "output_df = tf_idf_analysis(u)\n",
    "print (output_df)\n",
    "# Convert the DataFrame to a dictionary\n",
    "output_dict = output_df.to_dict('records')\n",
    "\n",
    "# Connect to the database\n",
    "mysql = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    passwd=\"\",\n",
    "    database=\"seopro\"\n",
    ")\n",
    "\n",
    "# Insert the data into the database\n",
    "mycursor = mysql.cursor()\n",
    "sql = \"INSERT INTO keywords (word, average_tfidf, max_tfidf, frequency, niche, uid) VALUES (%s, %s, %s, %s, %s, %s)\"\n",
    "for row in output_dict:\n",
    "    val = (row['word'], row['average_tfidf'], row['max_tfidf'], row['frequency'], niche, uid)\n",
    "    mycursor.execute(sql, val)\n",
    "mysql.commit()\n",
    "\n",
    "# Print a message to confirm that the data has been saved\n",
    "print(\"Data has been saved to the database.\")\n",
    "\n",
    "\n",
    "\n",
    "#try this if above is not working\n",
    "\n",
    "# import requests\n",
    "# import urllib\n",
    "# import pandas as pd\n",
    "# from requests_html import HTML\n",
    "# from requests_html import HTMLSession\n",
    "# import numpy as np\n",
    "# from fake_useragent import UserAgent\n",
    "# from urllib.request import Request, urlopen\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from nltk.data import url2pathname\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem.porter import *\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# stop_words = set(stopwords.words('english')) \n",
    "# stop_words.update(['le', 'eu', 'span', 'ago', 'pp', 'ue', 'div', 'src', 'page', 'egp', 'url', 'cdn', 'alt', 'com', 'net', 'org', 'cdn', 'img', 'google','eg','usd','http','https','net','us','eg'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import mysql.connector\n",
    "\n",
    "# # Connect to the database\n",
    "# mysql = mysql.connector.connect(\n",
    "#     host=\"localhost\",\n",
    "#     user=\"root\",\n",
    "#     password=\"\",\n",
    "#     database=\"seopro\"\n",
    "# )\n",
    "\n",
    "# # Create a cursor object\n",
    "# mycursor = mysql.cursor()\n",
    "\n",
    "# # Execute an SQL query to select the last row from the table\n",
    "# mycursor.execute(\"SELECT uid, niche FROM given_niche ORDER BY id DESC LIMIT 1\")\n",
    "\n",
    "# Fetch the last row from the table\n",
    "# niche_arr = mycursor.fetchone()\n",
    "\n",
    "# # # Convert niche[0] to a string\n",
    "# # niche= str(niche_arr[0])\n",
    "\n",
    "# # # Print the last row as a string\n",
    "# # print(niche)\n",
    "\n",
    "# # Extract the id and niche values from the row\n",
    "# uid = niche_arr[0]\n",
    "# niche = niche_arr[1]\n",
    "\n",
    "# # Print the niche and id values\n",
    "# print(niche)\n",
    "# print(uid)\n",
    "\n",
    "\n",
    "# def get_text(url):\n",
    "#     try:\n",
    "#         req = Request(url , headers={'User-Agent': 'Mozilla/5.0'})\n",
    "#         webpage = urlopen(req,timeout=5).read()\n",
    "#         soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "#         texts = soup.findAll(text=True)\n",
    "#         res=u\" \".join(t.strip() for t in texts if t.parent.name not in ['style', 'script', 'head', 'title', 'meta', '[document]'])\n",
    "#         return(res)\n",
    "#     except:\n",
    "#         return False\n",
    "    \n",
    "# \"\"\"Step 2: Get the URLs from competitors\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# def get_source(url):\n",
    "#     try:\n",
    "#         session = HTMLSession()\n",
    "#         response = session.get(url)\n",
    "\n",
    "#         return response\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(e)\n",
    "# def scrape_google(query):\n",
    "\n",
    "#     query = urllib.parse.quote_plus(query)\n",
    "#     response = get_source(\"https://www.google.com/search?q=\" + query)\n",
    "#     links = list(response.html.absolute_links)\n",
    "#     # google_domains = ('https://www.google.', \n",
    "#     #                   'https://google.', \n",
    "#     #                   'https://webcache.googleusercontent.', \n",
    "#     #                   'http://webcache.googleusercontent.', \n",
    "#     #                   'https://policies.google.',\n",
    "#     #                   'https://support.google.',\n",
    "#     #                   'https://maps.google.')\n",
    "    \n",
    "#     google_domains = ('https://www.google.', \n",
    "#                       'https://google.')\n",
    "     \n",
    "\n",
    "#     for url in links[:]:\n",
    "#         if url.startswith(google_domains):\n",
    "#             links.remove(url)\n",
    "\n",
    "#     return links\n",
    "\n",
    "# # niche = input(\"enter your niche\")\n",
    "# # scrape_google(niche)\n",
    "\n",
    "# \"\"\"Step 3: Analyse the text and get the most important words.\"\"\"\n",
    "\n",
    "# # niche = input(\"enter your niche: \")\n",
    "# links=scrape_google(niche)\n",
    "# print(links)\n",
    "# text=[]\n",
    "# for i in links:\n",
    "#   t=get_text(i)\n",
    "#   if t:\n",
    "#     text.append(t)\n",
    "\n",
    "#     word_tokens = word_tokenize(t)\n",
    "\n",
    "   \n",
    "#     filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    \n",
    "\n",
    "\n",
    "# p_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for word in filtered_sentence:\n",
    "#     # if len(word) >= 3:\n",
    "#     word+' --> '+p_stemmer.stem(word)\n",
    "\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "\n",
    "\n",
    "# for word in filtered_sentence:\n",
    "#     if len(word) >=4:\n",
    "#         word+' --> '+s_stemmer.stem(word)\n",
    "# u=p_stemmer.stem(word)\n",
    "# import mysql.connector\n",
    "\n",
    "# ########\n",
    "# # Modify the tf_idf_analysis function to return a DataFrame\n",
    "# def tf_idf_analysis(keyword):\n",
    "#     v = TfidfVectorizer(min_df=1,analyzer='word',ngram_range=(1,2),stop_words=list(stop_words))\n",
    "#     x = v.fit_transform(text)\n",
    "#     f = pd.DataFrame(x.toarray(), columns = v.get_feature_names_out())\n",
    "#     d=pd.concat([pd.DataFrame(f.mean(axis=0)),pd.DataFrame(f.max(axis=0))],axis=1)\n",
    "#     tf=pd.DataFrame((f>0).sum(axis=0))\n",
    "#     d=d.reset_index().merge(tf.reset_index(),on='index',how='left')\n",
    "#     d.columns=['word','average_tfidf','max_tfidf','frequency']\n",
    "#     d['frequency']=round((d['frequency']/len(text))*100)\n",
    "#     d['max_tfidf'] = d['max_tfidf'].round(3)\n",
    "#     d['average_tfidf'] = d['average_tfidf'].round(3)\n",
    "\n",
    "#     d= d[d['word'].str.isalpha()].sort_values('frequency',ascending=False).head(35)\n",
    "    \n",
    "#     return d\n",
    "\n",
    "\n",
    "# # Call the tf_idf_analysis function and store the output in a DataFrame variable\n",
    "# output_df = tf_idf_analysis(u)\n",
    "# print (output_df)\n",
    "# # Convert the DataFrame to a dictionary\n",
    "# output_dict = output_df.to_dict('records')\n",
    "\n",
    "# # Connect to the database\n",
    "# mysql = mysql.connector.connect(\n",
    "#     host=\"localhost\",\n",
    "#     user=\"root\",\n",
    "#     passwd=\"\",\n",
    "#     database=\"seopro\"\n",
    "# )\n",
    "\n",
    "# # Insert the data into the database\n",
    "# mycursor = mysql.cursor()\n",
    "# sql = \"INSERT INTO keywords (word, average_tfidf, max_tfidf, frequency, niche, uid) VALUES (%s, %s, %s, %s, %s, %s)\"\n",
    "# for row in output_dict:\n",
    "#     val = (row['word'], row['average_tfidf'], row['max_tfidf'], row['frequency'], niche, uid)\n",
    "#     mycursor.execute(sql, val)\n",
    "# mysql.commit()\n",
    "\n",
    "# # Print a message to confirm that the data has been saved\n",
    "# print(\"Data has been saved to the database.\")\n",
    "\n",
    "\n",
    "print(time.time()-t0)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "108095e740c160118120ad5e22811b4aca9f414a6f3c55c969835bc2c10848d1"
  },
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
