{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests-html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession\n",
    "import numpy as np\n",
    "from fake_useragent import UserAgent\n",
    "import re\n",
    "from urllib.request import Request, urlopen\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_text(url):\n",
    "    try:\n",
    "        req = Request(url , headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urlopen(req,timeout=5).read()\n",
    "        soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "        texts = soup.findAll(text=True)\n",
    "        res=u\" \".join(t.strip() for t in texts if t.parent.name not in ['style', 'script', 'head', 'title', 'meta', '[document]'])\n",
    "        return(res)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def get_source(url):\n",
    "    \"\"\"Return the source code for the provided URL. \n",
    "    Args: \n",
    "        url (string): URL of the page to scrape.\n",
    "    Returns:\n",
    "        response (object): HTTP response object from requests_html. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(url)\n",
    "\n",
    "       \n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "def scrape_google(query):\n",
    "\n",
    "    query = urllib.parse.quote_plus(query)\n",
    "    response = get_source(\"https://www.google.co.uk/search?q=\" + query)\n",
    "    links = list(response.html.absolute_links)\n",
    "    google_domains = ('https://www.google.', \n",
    "                      'https://google.', \n",
    "                      'https://webcache.googleusercontent.', \n",
    "                      'http://webcache.googleusercontent.', \n",
    "                      'https://policies.google.',\n",
    "                      'https://support.google.',\n",
    "                      'https://maps.google.')\n",
    "\n",
    "    for url in links[:]:\n",
    "        if url.startswith(google_domains):\n",
    "            links.remove(url)\n",
    "\n",
    "    return links\n",
    "\n",
    "scrape_google(\"cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.data import url2pathname\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from fake_useragent import UserAgent\n",
    "import re\n",
    "from urllib.request import Request, urlopen\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(len(stop_words))\n",
    "stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# example_sent = get_text('https://en.wikipedia.org/wiki/Machine_learning')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "links=scrape_google('cars')\n",
    "\n",
    "text=[]\n",
    "for i in links:\n",
    "  t=get_text(i)\n",
    "  if t:\n",
    "    text.append(t)\n",
    "\n",
    "    word_tokens = word_tokenize(t)\n",
    "# word_tokens = \n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens]\n",
    "\n",
    "    # print(word_tokens)\n",
    "    # print('---------------------------------')\n",
    "    # print(filtered_sentence)\n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    # print(word_tokens)\n",
    "    # print('---------------------------------')\n",
    "    # print(filtered_sentence)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "    print(word_tokens)\n",
    "    print('---------------------------------')\n",
    "    print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "for word in filtered_sentence:\n",
    "    if len(word) >= 3:\n",
    "        print(word+' --> '+p_stemmer.stem(word))\n",
    "        print(word+' --> '+s_stemmer.stem(word))\n",
    "u=p_stemmer.stem(word) + s_stemmer.stem(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def tf_idf_analysis(keyword):\n",
    "    v = TfidfVectorizer(min_df=1,analyzer='word',ngram_range=(1,2),stop_words=list(stop_words))\n",
    "    x = v.fit_transform(text)\n",
    "\n",
    "    f = pd.DataFrame(x.toarray(), columns = v.get_feature_names_out())\n",
    "    d=pd.concat([pd.DataFrame(f.mean(axis=0)),pd.DataFrame(f.max(axis=0))],axis=1)\n",
    "    \n",
    "    tf=pd.DataFrame((f>0).sum(axis=0))\n",
    "\n",
    "\n",
    "    d=d.reset_index().merge(tf.reset_index(),on='index',how='left')\n",
    "\n",
    "    d.columns=['word','average_tfidf','max_tfidf','frequency']\n",
    "\n",
    "    d['frequency']=round((d['frequency']/len(text))*100)\n",
    "\n",
    "    return(d)\n",
    "\n",
    "x= tf_idf_analysis(u)\n",
    "x = x[x['word'].str.isalpha() & (x['word'].str.len() >= 4)].sort_values('max_tfidf', ascending=False).head(35)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "# Download the pre-trained Word2Vec model\n",
    "model = api.load('word2vec-google-news-300')\n",
    "\n",
    "word_list = stemmed_words\n",
    "\n",
    "# Load stop words and create a lemmatizer object\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Preprocess the words in the list\n",
    "word_list_preprocessed = [preprocess(word) for word in word_list]\n",
    "\n",
    "# Define a function to find the alternative words for a word with high Word2Vec similarity scores\n",
    "def find_high_similarity_alternative_words(word, threshold=0.6):\n",
    "    alternatives = []\n",
    "    if word not in model:\n",
    "        return alternatives\n",
    "    for w, score in model.most_similar(word):\n",
    "        if score > threshold:\n",
    "            alternatives.append(w)\n",
    "    return alternatives\n",
    "\n",
    "# Find the higher ranking alternative words for each word in the list\n",
    "alternatives_dict = {}\n",
    "for word in word_list:\n",
    "    alternatives = find_high_similarity_alternative_words(word)\n",
    "    alternatives_dict[word] = alternatives\n",
    "\n",
    "# Print the higher ranking alternative words for each word in the list\n",
    "for word, alternatives in alternatives_dict.items():\n",
    "    print(word + ':', alternatives)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
